{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os \n",
    "import pandas as pd\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "from data_handler.models import Article\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sentiment.model import progress\n",
    "import plotly.express as px\n",
    "from sentiment.models import Category\n",
    "import sys\n",
    "from django.db.models import Avg, Count, Min, Sum\n",
    "from datetime import timedelta, date\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "all_articles = Article.objects.all()\n",
    "df = pd.read_csv(\"cleaned_docs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...Article 33197/33197\r"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "length = len(all_articles)\n",
    "notfound = []\n",
    "for article in all_articles:\n",
    "    index += 1\n",
    "    progress(index, length, status=\"Article {}/{}\".format(index, length))\n",
    "    row = df.loc[df['headlines'] == article.headline]\n",
    "    value = row['clean_docs'].tolist()\n",
    "    if not value:\n",
    "        notfound.append(article)\n",
    "        continue\n",
    "    article.tokens = value[0]\n",
    "    article.save()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handler.models import Category\n",
    "\n",
    "negativ = [entry['word'] for entry in list(Category.objects.get(name=\"Negativ\").words.all().values('word'))]\n",
    "neg = [entry['word'] for entry in list(Category.objects.get(name=\"Ngtv\").words.all().values('word'))]\n",
    "\n",
    "positiv = [entry['word'] for entry in list(Category.objects.get(name=\"Positiv\").words.all().values('word'))]\n",
    "pos = [entry['word'] for entry in list(Category.objects.get(name=\"Pstv\").words.all().values('word'))]\n",
    "\n",
    "ngtv = set(negativ+neg)\n",
    "pstv = set(positiv+pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...Article of 33197/33197\r"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "index = 0\n",
    "length = len(all_articles)\n",
    "cur_art = \"\"\n",
    "for article in all_articles:\n",
    "    index += 1\n",
    "    cur_art = article\n",
    "    progress(index, length, status=\"Article of {}/{}\".format(index, length))\n",
    "    tokens = word_tokenize(article.tokens)\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for token in tokens:\n",
    "        token = token.upper()\n",
    "        if token in ngtv:\n",
    "            neg += 1\n",
    "        elif token in pstv:\n",
    "            pos += 1\n",
    "    article.smarter_negative_words = neg\n",
    "    article.smarter_positive_words = pos\n",
    "    article.save()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment.model import preprocess\n",
    "\n",
    "class ArticleHeadlineIterator(object):\n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "    def __iter__(self):\n",
    "        for article in self.articles:\n",
    "            yield article.headline\n",
    "\n",
    "class ArticleContentIterator(object):\n",
    "    def __init__(self, articles):\n",
    "        self.articles = articles\n",
    "    def __iter__(self):\n",
    "        for article in self.articles:\n",
    "            row = article.headline + \". \" + article.contents\n",
    "            yield row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_art.smarter_negative_words\n",
    "cur_art.smarter_positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defiant new Tory MPs say they will vote for Brexit\n"
     ]
    }
   ],
   "source": [
    "print(cur_art.headline)\n",
    "art = df.loc[df['headlines']==cur_art.headline]\n",
    "# print(art)\n",
    "# cur_art.tokens = art['clean_docs'].tolist()[0]\n",
    "# cur_art.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.0012749195098876952\n"
     ]
    }
   ],
   "source": [
    "contentit = ArticleContentIterator(notfound)\n",
    "\n",
    "clean_articles = preprocess(contentit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineit = ArticleHeadlineIterator(notfound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for headline in headlineit:\n",
    "    art = Article.objects.filter(headline=headline).first()\n",
    "    try:\n",
    "        art.delete()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df.loc[df['headlines']=='How London\\'s loss is Dublin\\'s gain after Brexit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1008 elements, new values have 1 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a08562dc21b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mautocorrelate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-a08562dc21b4>\u001b[0m in \u001b[0;36mautocorrelate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final year project/sentiment_analysis/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/final year project/sentiment_analysis/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/final year project/sentiment_analysis/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    181\u001b[0m             raise ValueError(\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;34m\"values have {new} elements\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 1008 elements, new values have 1 elements"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class DataFrameCreator(object):\n",
    "    def __init__(self, stocks):\n",
    "        self.stocks = stocks\n",
    "\n",
    "    def __iter__(self):\n",
    "        # get the stocks first and iterate through them\n",
    "        for stock in self.stocks:\n",
    "            # get sentiment of articles for that day\n",
    "            date = stock.date\n",
    "            arts = Article.objects.filter(date_written=date)\n",
    "            if arts:\n",
    "                total_length = arts.aggregate(total_length=Sum('length'))['total_length']\n",
    "                smarter_sum = arts.aggregate(total_neg_words=Sum('smarter_negative_words'))['total_neg_words']\n",
    "                negative_sentiment = smarter_sum/total_length\n",
    "                yield {'date':date, 'sentiment':negative_sentiment, 'return': stock.log_return()}\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "def autocorrelate_data():\n",
    "    stocks = StockPrice.objects.all()\n",
    "    dfc = DataFrameCreator(stocks)\n",
    "    df = pd.DataFrame.from_dict(dfc)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    df = df[(df.T != 0).all()]\n",
    "    df.index = ['date']\n",
    "    print(df.head())\n",
    "    arr = np.asarray(df)\n",
    "    print(arr)\n",
    "    # create the VAR model\n",
    "    model = VAR(endog=arr)\n",
    "    model_fit = model.fit()\n",
    "    model.plot()\n",
    "    \n",
    "autocorrelate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
